{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33db9cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from peft import LoraConfig, get_peft_config, PeftModel\n",
    "from sklearn.model_selection import train_test_split \n",
    "from peft import LoraConfig, get_peft_model\n",
    "from Retrieval_system import Retrieval\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from pincone import Pincone_vectorStore\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f39ff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connect to <pinecone.db_data.index.Index object at 0x7f9fd2d549a0>\n"
     ]
    }
   ],
   "source": [
    "Device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "index= Pincone_vectorStore()\n",
    "Embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\", model_kwargs={\"device\": Device})\n",
    "vector_store = PineconeVectorStore(embedding=Embeddings,index=index)\n",
    "retrieval_obj = Retrieval(device=Device, index=index,Embeddings=Embeddings,vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8cc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"/home/shegun93/ITS/Fine-tunning/nairsV1\"\n",
    "ATTN_IMPLEMENTATION = \"flash_attention_2\" if (is_flash_attn_2_available() and (torch.cuda.get_device_capability(0)[0] >= 8)) else \"sdpa\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#MODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68efefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_quantization_config = True\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e18b2ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: flash_attention_2\n",
      "[INFO] Using model_id: /home/shegun93/ITS/Fine-tunning/nairsV1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shegun93/anaconda3/envs/RAGs/lib/python3.9/site-packages/transformers/quantizers/auto.py:195: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"[INFO] Using attention implementation: {ATTN_IMPLEMENTATION}\")\n",
    "print(f\"[INFO] Using model_id: {MODEL_ID}\")\n",
    "Model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config if use_quantization_config else None,                                             \n",
    "    attn_implementation=ATTN_IMPLEMENTATION,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b01c8b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mcq(doc_text):\n",
    "    lines = [line.strip() for line in doc_text.splitlines() if line.strip()]\n",
    "\n",
    "    question = None\n",
    "    options = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"Question:\"):\n",
    "            question = line.replace(\"Question:\", \"\").strip()\n",
    "\n",
    "        elif len(line) > 2 and line[1] == \".\" and line[0] in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            options[line[0]] = line[2:].strip()\n",
    "\n",
    "    return question, options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8baa60ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=Model, \n",
    "                tokenizer=tokenizer, max_new_tokens = 150,\n",
    "                return_full_text = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7378ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_student_answer(query):\n",
    "    retriever, _ = retrieval_obj.get_retrieval() \n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    if not docs:\n",
    "        return \"No relevant context found.\"\n",
    "    doc = docs[0]\n",
    "    question, options = parse_mcq(doc.page_content)\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    for k, v in options.items():\n",
    "        print(f\"{k}. {v}\")\n",
    "\n",
    "    user_answer = input(\"\\nEnter your answer (A/B/C/D): \").strip().upper()\n",
    "    if user_answer not in options:\n",
    "        return \"Invalid selection. Please choose A, B, C, or D.\"\n",
    "\n",
    "\n",
    "    formatted_options = \"\\n\".join([f\"{k}. {v}\" for k, v in options.items()])\n",
    "    prompt_text = (\n",
    "    f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    f\"You are a Pedagogical Expert. Your task is to diagnose student errors based ONLY on the provided CONTEXT.\\n\"\n",
    "    f\"RULES:\\n\"\n",
    "    f\"1. Compare the Student Answer to the CORRECT answer found in the CONTEXT.\\n\"\n",
    "    f\"2. Identify the specific misunderstanding (e.g., confusing two different proofs of relativity).\\n\"\n",
    "    f\"3. Provide a 'Socratic Hint' that points out a flaw in their logic without naming the correct option.\\n\"\n",
    "    f\"4. BE FACTUALLY ACCURATE. Do not invent details.<|eot_id|>\"\n",
    "    f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "    f\"CONTEXT: {doc.page_content}\\n\"\n",
    "    f\"QUESTION: {question}\\n\"\n",
    "    f\"OPTIONS: {formatted_options}\\n\"\n",
    "    f\"STUDENT ANSWER: {user_answer}<|eot_id|>\"\n",
    "    f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    f\"<diagnosis>\"\n",
    ")\n",
    "    full_response = llm.invoke(prompt_text)\n",
    "    if \"<hint>\" in full_response:\n",
    "        hint_only = full_response.split(\"<hint>\")[-1].strip()\n",
    "        return hint_only\n",
    "    return full_response\n",
    "    #response = llm.invoke(prompt_text)\n",
    "    #return f\"<thinking>{response}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c22dd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What observable phenomenon confirmed Einstein's theory of general relativity in 1919?\n",
      "A. Gravitational redshift\n",
      "B. Time dilation in GPS satellites\n",
      "C. Bending of starlight by the Sun\n",
      "D. Black hole mergers\n",
      "Gravitational redshift occurs when light escapes from a massive object's gravitational field and its frequency is lowered due to the increased distance from the source. This phenomenon was observed and confirmed in 1919 during a solar eclipse, when scientists measured the redshift of light passing near the Sun.\n"
     ]
    }
   ],
   "source": [
    "result = evaluate_student_answer(\"What is the Principle of Relativity\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d328f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER = \"Tell me the principle of relativity\"\n",
    "# Prompt = f\"\"\"[SYSTEM] You are an AI Tutor. Identify the student's misconception and provide a helpful hint.\n",
    "# Q: {USER}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e779de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27ae5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2611e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6cf634",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_student_answer(\"WHAT IS Thermodynaics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba63ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USER = \"Tell me the principle of relativity\"\n",
    "Prompt = f\"\"\"[SYSTEM] You are an AI Tutor. Identify the student's misconception and provide a helpful hint.\n",
    "Q: {USER}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525d950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be1993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820e721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c555ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e4119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(Model.device)\n",
    "    # with torch.no_grad():\n",
    "    #     output_tokens = Model.generate(\n",
    "    #         **inputs,\n",
    "    #         max_new_tokens=150,    \n",
    "    #         temperature=0.1,      \n",
    "    #         repetition_penalty=1.2, \n",
    "    #         do_sample=True,\n",
    "    #         eos_token_id=tokenizer.eos_token_id,\n",
    "    #         pad_token_id=tokenizer.eos_token_id\n",
    "    #     )\n",
    "    # input_length = inputs.input_ids.shape[1]\n",
    "    # generated_text = tokenizer.decode(output_tokens[0][input_length:], skip_special_tokens=True)\n",
    "    # full_response = \"<thinking>\" + generated_text\n",
    "    # clean_output = full_response.split(\"Q:\")[0].split(\"Question:\")[0].split(\"[INST]\")[0].strip()\n",
    "    # return clean_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
