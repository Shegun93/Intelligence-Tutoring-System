{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db9cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from peft import LoraConfig, get_peft_config, PeftModel\n",
    "from sklearn.model_selection import train_test_split \n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8cc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"./Fine-tunning/nairsV1\"\n",
    "ATTN_IMPLEMENTATION = \"flash_attention_2\" if (is_flash_attn_2_available() and (torch.cuda.get_device_capability(0)[0] >= 8)) else \"sdpa\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68efefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_quantization_config = False\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18b2ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shegun93/anaconda3/envs/RAGs/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: flash_attention_2\n",
      "[INFO] Using model_id: meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ecf76ce01545588413e7c3d477b0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"[INFO] Using attention implementation: {ATTN_IMPLEMENTATION}\")\n",
    "print(f\"[INFO] Using model_id: {MODEL_ID}\")\n",
    "Model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config if use_quantization_config else None,                                             \n",
    "    attn_implementation=ATTN_IMPLEMENTATION,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53788ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaFlashAttention2(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d328f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER = \"Tell me the principle of relativity\"\n",
    "# Prompt = f\"\"\"[SYSTEM] You are an AI Tutor. Identify the student's misconception and provide a helpful hint.\n",
    "# Q: {USER}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b88a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb1412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0e779de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connect to <pinecone.db_data.index.Index object at 0x7f3e540dcd90>\n"
     ]
    }
   ],
   "source": [
    "from Retrieval_system import Retrieval\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pincone import Pincone_vectorStore\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "Device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "index= Pincone_vectorStore()\n",
    "Embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\", model_kwargs={\"device\": Device})\n",
    "vector_store = PineconeVectorStore(embedding=Embeddings,index=index)\n",
    "retrieval_obj = Retrieval(device=Device, index=index,Embeddings=Embeddings,vector_store=vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e27ae5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mcq(doc_text):\n",
    "    lines = [line.strip() for line in doc_text.splitlines() if line.strip()]\n",
    "\n",
    "    question = None\n",
    "    options = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"Question:\"):\n",
    "            question = line.replace(\"Question:\", \"\").strip()\n",
    "\n",
    "        elif len(line) > 2 and line[1] == \".\" and line[0] in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            options[line[0]] = line[2:].strip()\n",
    "\n",
    "    return question, options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff2611e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_student_answer(query):\n",
    "    retriever, _ = retrieval_obj.get_retrieval() \n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    if not docs:\n",
    "        return \"No relevant context found.\"\n",
    "    doc = docs[0]\n",
    "    question, options = parse_mcq(doc.page_content)\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    for k, v in options.items():\n",
    "        print(f\"{k}. {v}\")\n",
    "\n",
    "    user_answer = input(\"\\nEnter your answer (A/B/C/D): \").strip().upper()\n",
    "    if user_answer not in options:\n",
    "        return \"Invalid selection. Please choose A, B, C, or D.\"\n",
    "\n",
    "\n",
    "    formatted_options = \"\\n\".join([f\"{k}. {v}\" for k, v in options.items()])\n",
    "    prompt_text = (\n",
    "        f\"<s>[INST] <<SYS>>\\n\"\n",
    "        f\"You are an AI Intelligent Tutoring System. Analyze the error in <thinking> tags, then provide a hint.\\n\"\n",
    "        f\"<</SYS>>\\n\\n\"\n",
    "        f\"Q: {question}\\n{formatted_options}\\n\"\n",
    "        f\"My Answer: {user_answer} [/INST]\\n\"\n",
    "        f\"<thinking>\" \n",
    "    )\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(Model.device)\n",
    "    with torch.no_grad():\n",
    "        output_tokens = Model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,    \n",
    "            temperature=0.1,      \n",
    "            repetition_penalty=1.2, \n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    generated_text = tokenizer.decode(output_tokens[0][input_length:], skip_special_tokens=True)\n",
    "    full_response = \"<thinking>\" + generated_text\n",
    "    clean_output = full_response.split(\"Q:\")[0].split(\"Question:\")[0].split(\"[INST]\")[0].strip()\n",
    "    return clean_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e6cf634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Which process violates the Second Law of Thermodynamics?\n",
      "A. Heat flowing from hot to cold\n",
      "B. Spontaneous decrease in entropy\n",
      "C. Energy conservation in a closed system\n",
      "D. Isothermal expansion of a gas\n",
      "<thinking>Ah, an interesting question! Let me analyze your answer...</thinking>\n",
      "\n",
      "Great job recognizing that option B is the correct answer! The Second Law of Thermodynamics states that \"in any spontaneous process, the total entropy of a closed system will always increase over time.\" Option B correctly identifies that a spontaneous decrease in entropy would violate this law.\n",
      "\n",
      "Here's a hint for future questions: When analyzing thermodynamic processes, it can be helpful to break them down into smaller components and consider their individual properties before making a decision. In this case, the fact that the process involves a spontaneous decrease in entropy was crucial in determining the correct answer. Keep\n",
      "<thinking>Ah, an interesting question! Let me analyze your answer...</thinking>\n",
      "\n",
      "Great job recognizing that option B is the correct answer! The Second Law of Thermodynamics states that \"in any spontaneous process, the total entropy of a closed system will always increase over time.\" Option B correctly identifies that a spontaneous decrease in entropy would violate this law.\n",
      "\n",
      "Here's a hint for future questions: When analyzing thermodynamic processes, it can be helpful to break them down into smaller components and consider their individual properties before making a decision. In this case, the fact that the process involves a spontaneous decrease in entropy was crucial in determining the correct answer. Keep\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_student_answer(\"WHAT IS Thermodynaics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba63ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USER = \"Tell me the principle of relativity\"\n",
    "Prompt = f\"\"\"[SYSTEM] You are an AI Tutor. Identify the student's misconception and provide a helpful hint.\n",
    "Q: {USER}\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
